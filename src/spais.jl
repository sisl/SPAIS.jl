function StatebasedProposalAIS(;agent::PolicyParams,
    likelihood::ValidationLikelihood,
    f_target,
    N,
    Î”N,
    max_steps,
    a_opt::NamedTuple=(;), 
    log::NamedTuple=(;), 
    required_columns=[],
    target_ess=nothing,
    name = "spais",
    kwargs...)

    ğ’« = (;prev_buffer=nothing, prev_target_log_ws=nothing, prev_returns=nothing,likelihood=likelihood, f_target=[f_target], target_ess=target_ess)
    required_columns = unique([required_columns..., :logprob, :return, :traj_importance_weight])
    train_params = TrainingParams(
        loss=msa_loss,
        optimizer=Flux.Optimiser(Flux.ClipValue(1f0), Flux.Adam(3f-4)),
        batch_size=256,
        epochs=1,
        a_opt...
    )
    #log=LoggerParams(;dir = "log/$name", period=1000, log...),

    ValidationSolver(;agent,
                     ğ’«=ğ’«,
                     N=N,
                     Î”N=Î”N,
                     training_buffer_size=Î”N*max_steps,
                     required_columns,
                     training_type=:spais,
                     log=LoggerParams(;dir = "log/$name", log...),
                     a_opt=train_params,
                     kwargs...)

end

function msa_loss(Ï€, ğ’«, ğ’Ÿ; info = Dict())
    # Compute the log probability
    new_probs = logpdf(Ï€, ğ’Ÿ[:s], ğ’Ÿ[:a])
    
    # Out of caution, remove any NaNs or Infs
    new_probs = new_probs[.!isnan.(new_probs) .& .!isinf.(new_probs)]

    ignore_derivatives() do
        info[:kl] = mean(ğ’Ÿ[:logprob][.!isnan.(new_probs) .& .!isinf.(new_probs)] .- new_probs)
    end 

    -mean(new_probs)
end

function get_values_from_buffer(ğ’Ÿ::ExperienceBuffer, key::Symbol)
    eps = episodes(ğ’Ÿ)
    values = [ğ’Ÿ[key][1, ep[1]] for ep in eps]
    return values
end

function update_likelihood(ğ’®::ValidationSolver, ğ’Ÿ)
    traj_weights = get_values_from_buffer(ğ’Ÿ, :traj_importance_weight)
    log_ws = log.(traj_weights)
    target_ess = ğ’®.ğ’«[:target_ess]
    returns = get_values_from_buffer(ğ’Ÿ, :return)
    â„“ = ğ’®.ğ’«[:likelihood]
    N = ğ’®.Î”N
    objective(t) = ess(exp.(log_ws .+ map(r->logdensity(â„“, r, t), returns)))/N - target_ess

    try
        res = optimize(objective, 1e-3, â„“.Î²)
        return res.minimizer
    catch e
        return â„“.Î²
    end
end

function metropolis_hastings_step(ğ’®::ValidationSolver, ğ’Ÿ; info=Dict())
    likelihood = ğ’®.ğ’«[:likelihood]
    prev_buffer = ğ’®.ğ’«[:prev_buffer]
    prev_target_log_ws = ğ’®.ğ’«[:prev_target_log_ws]
    prev_returns = ğ’®.ğ’«[:prev_returns]

    rs = get_values_from_buffer(ğ’Ÿ, :return)
    ws = get_values_from_buffer(ğ’Ÿ, :traj_importance_weight)
    ls = map(likelihood, rs)
    target_log_ws = log.(ws) .+ ls

    info[:ess] = ess(exp.(target_log_ws))
    info[:mean_targ_weights] = mean(exp.(target_log_ws))
    info[:std_targ_weights] = std(exp.(target_log_ws))

    if isnothing(prev_buffer)
        ğ’®.ğ’« = merge(ğ’®.ğ’«, (;prev_buffer=deepcopy(ğ’Ÿ), prev_target_log_ws=target_log_ws, prev_returns=rs))
        return ğ’Ÿ
    end

    # If a target effective sample size is specified,
    # update the likelihood temperature (smoothing factor) and the target log weights
    if ~isnothing(ğ’®.ğ’«[:target_ess])
        if info[:ess]/ğ’®.Î”N >= ğ’®.ğ’«[:target_ess]
            Î²_new = update_likelihood(ğ’®, ğ’Ÿ)
            likelihood.Î² = Î²_new

            # update the target log weights with the new likelihood
            target_log_ws = log.(ws) .+ map(likelihood, rs)
            prev_target_log_ws = log.(get_values_from_buffer(prev_buffer, :traj_importance_weight)) .+ map(likelihood, prev_returns)
        end
        info[:temp] = likelihood.Î²
    end

    # Compute the acceptance probability
    log_acceptance_probs = min.(0.0, target_log_ws .- prev_target_log_ws)

    # Accept or reject the samples
    accept = log.(rand(length(log_acceptance_probs))) .< log_acceptance_probs
    info[:accept_rate] = mean(accept)

    # Construct the new trajectory buffer
    new_buffer = buffer_like(ğ’Ÿ, capacity=ğ’®.training_buffer_size, device=device(ğ’®.agent.Ï€))
    new_eps = collect(episodes(ğ’Ÿ))
    prev_eps = collect(episodes(prev_buffer))

    accepted_eps = get_episodes(ğ’®.ğ’Ÿ, new_eps[accept])
    rejected_eps = get_episodes(prev_buffer, prev_eps[.!accept])
    
    # Check that the accepted_eps and rejected_eps are not empty
    if length(accepted_eps) > 0
        push!(new_buffer, accepted_eps)
    end
    if length(rejected_eps) > 0
        push!(new_buffer, rejected_eps)
    end
    
    # Merge together the target log weights, traj returns and the buffer
    new_target_log_ws = vcat(target_log_ws[accept], prev_target_log_ws[.!accept])
    new_returns = vcat(rs[accept], prev_returns[.!accept])
    ğ’®.ğ’« = merge(ğ’®.ğ’«, (;prev_buffer=new_buffer, prev_target_log_ws=new_target_log_ws, prev_returns=new_returns))

    return new_buffer
end

function spais_training(ğ’®::ValidationSolver, ğ’Ÿ)
    info = Dict()

    # Perform Metropolis-Hastings step on target distribution
    ğ’Ÿ = metropolis_hastings_step(ğ’®, ğ’Ÿ, info=info)    

    # Train Actor on updated trajectories
    Ï€ = ğ’®.agent.Ï€
    batch_train!(actor(Ï€), ğ’®.a_opt, ğ’®.ğ’«, deepcopy(ğ’Ÿ), info=info, Ï€_loss=Ï€)
    
    info
end